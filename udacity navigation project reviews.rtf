{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fnil\fcharset0 Menlo-Bold;
}
{\colortbl;\red255\green255\blue255;\red36\green36\blue36;\red255\green255\blue255;\red70\green81\blue90;
\red35\green46\blue57;\red21\green163\blue221;\red240\green240\blue240;}
{\*\expandedcolortbl;;\cssrgb\c18824\c18824\c18824;\cssrgb\c100000\c100000\c100000;\cssrgb\c34510\c39216\c42745;
\cssrgb\c18039\c23922\c28627;\cssrgb\c784\c70196\c89412;\cssrgb\c95294\c95294\c95294;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl640\sa230\qc\partightenfactor0

\f0\fs40 \cf2 \cb3 \expnd0\expndtw0\kerning0
Meets Specifications\cb1 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\fs28 \cf4 \cb3 Congratulations in meeting the specifications of the project! I hope you have enjoyed this project.\cb1 \uc0\u8232 \cb3 This project had discrete action space. Next, you will work with continuous action control space of the environment.\cb1 \uc0\u8232 \cb3 I hope that you're enjoying this nanodegree and excited to move ahead! Good luck!\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf4 \cb3 Please note that this review is in the continuation of the previous review. I hope you will find it helpful :)\cb1 \
\pard\pardeftab720\sl320\partightenfactor0
\cf4 \
\pard\pardeftab720\sl560\sa230\partightenfactor0

\f1\b\fs36 \cf5 \cb3 Training Code\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0\fs28 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The repository (or zip file) includes functional, well-documented, and organized code for training the agent.\cb1 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f0\b0 \cf4 \cb3 You can easily write the codes into Python files and make this program as a command line application. You can also then add hyper-parameters arguments by the use of the argparser in order to easily tune the values of the hyper-parameters through command line. Check out this tutorial on argparse\'a0{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=rnatu3xxVQE"}}{\fldrslt \cf6 https://www.youtube.com/watch?v=rnatu3xxVQE}}\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf4 \cb3 Check out this blog\'a0{\field{\*\fldinst{HYPERLINK "https://www.pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/"}}{\fldrslt \cf6 https://www.pyimagesearch.com/2018/03/12/python-argparse-command-line-arguments/}}\cb1 \
\pard\pardeftab720\sl320\partightenfactor0
\cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The code is written in PyTorch and Python 3.\cb1 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b0 \cf4 \cb3 If you like, you can convert all your Pytorch codes into tensorflow and do comparison analysis. This will be great after-project exercise to decide which framework to work on for the deep learning problems.\cb1 \
\pard\pardeftab720\sl320\partightenfactor0
\cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The submission includes the saved model weights of the successful agent.\cb1 \
\pard\pardeftab720\sl400\partightenfactor0

\f0\b0 \cf4 \cb3 You should save the checkpoints by string formatting at several intervals of the total episodes and then compare how much the agent has learned about the task across the checkpoint intervals. For example, at checkpoint of 500th episode, it is expected the agent to perform better than at at checkpoint of 100th episode during the inference.\cb1 \
\pard\pardeftab720\sl320\partightenfactor0
\cf4 \
\pard\pardeftab720\sl560\sa230\partightenfactor0

\f1\b\fs36 \cf5 \cb3 README\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0\fs28 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The GitHub (or zip file) submission includes a\'a0
\f2\fs25\fsmilli12600 \cf2 \cb7 README.md
\f1\fs28 \cf5 \cb3 \'a0file in the root of the repository.\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The README describes the the project environment details (i.e., the state and action spaces, and when the environment is considered solved).\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The README has instructions for installing dependencies or downloading needed files.\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The README describes how to run the code in the repository, to train the agent. For additional resources on creating READMEs or using Markdown, see\'a0{\field{\*\fldinst{HYPERLINK "https://www.udacity.com/courses/ud777"}}{\fldrslt \cf6 here}}\'a0and\'a0{\field{\*\fldinst{HYPERLINK "https://guides.github.com/features/mastering-markdown/"}}{\fldrslt \cf6 here}}.\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0 \cf4 \
\pard\pardeftab720\sl560\sa230\partightenfactor0

\f1\b\fs36 \cf5 \cb3 Report\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0\fs28 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The submission includes a file in the root of the GitHub repository or zip file (one of\'a0
\f2\fs25\fsmilli12600 \cf2 \cb7 Report.md
\f1\fs28 \cf5 \cb3 ,\'a0
\f2\fs25\fsmilli12600 \cf2 \cb7 Report.ipynb
\f1\fs28 \cf5 \cb3 , or\'a0
\f2\fs25\fsmilli12600 \cf2 \cb7 Report.pdf
\f1\fs28 \cf5 \cb3 ) that provides a description of the implementation.\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The report clearly describes the learning algorithm, along with the chosen hyperparameters. It also describes the model architectures for any neural networks.\cb1 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f0\b0 \cf4 \cb3 You should also define the Q-learning algorithm using the concepts of the Bellman Equation, Temporal Difference Learning. You can also include the below agent's description;\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf4 \cb3 What is the experience replay and what's the use of it? You can refer to this lesson video\'a0{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=wX_-SZG-YMQ"}}{\fldrslt \cf6 https://www.youtube.com/watch?v=wX_-SZG-YMQ}}\cb1 \uc0\u8232 \cb3 What is the Fixed Q targets that you have implemented, the parameters that you have used like tau? How this technique helps. You can refer to this lesson video\'a0{\field{\*\fldinst{HYPERLINK "https://www.youtube.com/watch?v=SWpyiEezfp4"}}{\fldrslt \cf6 https://www.youtube.com/watch?v=SWpyiEezfp4}}\cb1 \
\pard\pardeftab720\sl320\partightenfactor0
\cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 A plot of rewards per episode is included to illustrate that the agent is able to receive an average reward (over 100 episodes) of at least +13. The submission reports the number of episodes needed to solve the environment.\cb1 \
\pard\pardeftab720\sl320\partightenfactor0

\f0\b0 \cf4 \
\pard\pardeftab720\sl400\partightenfactor0

\f1\b \cf5 \cb3 The submission has concrete future ideas for improving the agent's performance.\cb1 \
\pard\pardeftab720\sl400\sa300\partightenfactor0

\f0\b0 \cf4 \cb3 I'd highly recommend to use Convolution Neural Nets in your Q-learning's neural network model when using the image pixel values as state. CNNs preserve the spatial information of the image. You can use the pixel based environment given in the classroom.\cb1 \
\cb3 Here's the nice description of benefit for using Dueling DQN\'a0{\field{\*\fldinst{HYPERLINK "https://www.quora.com/Why-does-a-dueling-network-for-deep-reinforcement-learning-work"}}{\fldrslt \cf6 https://www.quora.com/Why-does-a-dueling-network-for-deep-reinforcement-learning-work}}\cb1 \
\pard\pardeftab720\sl400\partightenfactor0
\cf4 \cb3 Here's the description of Double DQN\'a0{\field{\*\fldinst{HYPERLINK "https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/"}}{\fldrslt \cf6 https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/}}\cb1 \
}